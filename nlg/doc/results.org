#+title: 实验结果图
#+date: Mon Feb 27 09:41:12 2023
#+author: Zi Liang
#+email: liangzid@stu.xjtu.edu.cn
#+latex_class: elegantpaper

* 提升效果-验证想法的章节
** vary epochs 

|---------+--------|
| items   | values |
|---------+--------|
| dataset |    e2e |
| dropout |    0.6 |
| lr      |   8e-5 |
|---------+--------|

eval on last 100 samples.

results:

|-----------+--------+---------|
|     epoch |   BLEU | rouge-L |
|-----------+--------+---------|
|         1 | 0.3424 |  0.3930 |
|         2 | 0.3797 |  0.4189 |
|         3 | 0.4194 |  0.3982 |
|         4 | 0.4146 |  0.4139 |
|         5 | 0.3650 |  0.4098 |
|   finally | 0.3650 |  0.4090 |
|   bestval | 0.0926 |  0.2583 |
|  trainmin | 0.4093 |  0.4190 |
| w.o. EmRs |        |         |
|-----------+--------+---------|

结论：存在明显的过拟合现象。但是总体效果并不佳.
应该测试：
1. dropout too large?
2. other things...

** 测试采样的测试集是否合理
using trainmin in chapter =vary epochs=

|------------------+--------+---------|
| key              |   BLEU | rouge-L |
|------------------+--------+---------|
| last 100         | 0.4093 |  0.4190 |
| first 100        | 0.4872 |  0.4843 |
| 500-600          | 0.4358 |  0.4386 |
| overall          | 0.4653 |  0.4452 |
|------------------+--------+---------|
| all-final(dr0.6) | 0.4531 |  0.4430 |
| all-vanilla      | 0.6381 |  0.4871 |
|------------------+--------+---------|

train model seems better than finally ckpt.

结论：应该使用全部的测试集进行效果调优。部分数据集的结果很不准。

** vary dropout Training

吸取了上次的结果，这次采用所有的测试集进行实验

ckpt采用的是train min。$*\rightarrow$ 不是很合理，更改为finally继续试验

past results in trainmin ckpt:

|-------------+--------+---------+---------+-----------|
|     dropout |   BLEU | rouge-L | ep2BLEU | ep2rougeL |
|-------------+--------+---------+---------+-----------|
|         0.2 | 0.3908 |  0.4111 |  0.3978 |    0.4160 |
|         0.3 | 0.4732 |  0.4542 |  0.4607 |    0.4438 |
|         0.4 | 0.4547 |  0.4474 |  0.4800 |    0.4457 |
|         0.5 | 0.4297 |  0.4183 |  0.4351 |    0.4177 |
|         0.6 | 0.4531 |  0.4430 |       - |         - |
|         0.7 | 0.4560 |  0.4333 |  0.4126 |    0.4365 |
|         0.8 | 0.4441 |  0.4369 |  0.4395 |    0.4259 |
| all-vanilla | 0.6381 |  0.4871 |       - |         - |
|-------------+--------+---------+---------+-----------|

new results in finally ckpt:

|-------------+--------+---------+---------+-----------|
|     dropout |   BLEU | rouge-L | ep2BLEU | ep2rougeL |
|-------------+--------+---------+---------+-----------|
|         0.2 | 0.4154 |  0.4186 |  0.3978 |    0.4160 |
|         0.3 | 0.4267 |  0.4155 |  0.4607 |    0.4438 |
|         0.4 | 0.5002 |  0.4632 |  0.4800 |    0.4457 |
|         0.5 | 0.4726 |  0.4331 |  0.4351 |    0.4177 |
|         0.6 |        |         |       - |         - |
|         0.7 | 0.5049 |  0.4594 |  0.4126 |    0.4365 |
|         0.8 | 0.4477 | 0.4412  |  0.4395 |    0.4259 |
| all-vanilla |        |         |       - |         - |
|-------------+--------+---------+---------+-----------|


 +测试结果：dropout选择0.7左右较佳+

测试了epoch2的结果，发现结果没有这么简单。目前认为：
1. dropout较小的模型可以较快的地收敛， +但是更容易过拟合+ 。
2. 目前从效果上来看，反而是dropout为0.4的模型获得了最好的效果。真是神奇。但是为了训练的稳定性，（代码已经跑上了），所以主要还是采用dropout=0.7进行后续的noise测试


现在做dpr=0.4上的vary epoch实验

|------------+--------+--------+--------------------------|
| eposide    |   BLEU | rougeL |                          |
|------------+--------+--------+--------------------------|
| ep0        | 0.3145 | 0.4144 |                          |
| ep1        | 0.4027 | 0.4189 |                          |
| ep2        | 0.4800 | 0.4457 | ? 后续将会衡量其可复现性 |
| ep3        | 0.4683 | 0.4374 |                          |
| ep4        | 0.5002 | 0.4632 |                          |
| finally    | 0.5002 | 0.4629 |                          |
| valmin     | 0.0585 | 0.2354 |                          |
| trainmodel | 0.4547 | 0.4473 |                          |
|------------+--------+--------+--------------------------|

** The effectiveness of noise

dropout=0.7

using ckpt finally
|-----------------+--------+---------+---------+-----------|
| noise threshold |   BLEU | rouge-L | BLEUep2 | rougeLep2 |
|-----------------+--------+---------+---------+-----------|
|             0.1 |        |         |         |           |
|             0.2 | 0.4560 |  0.4333 |  0.4126 |    0.4365 |
|             0.3 | 0.4710 |  0.4377 |  0.4522 |    0.4440 |
|             0.4 | 0.4412 |  0.4401 |  0.4775 |    0.4461 |
|             0.5 | 0.4538 |  0.4428 |  0.4514 |    0.4455 |
|             0.6 | 0.4941 |  0.4450 |  0.4146 |    0.4229 |
|            0.7 | 0.5064 |  0.4618 |  0.5131 |    0.4639 |
|             0.8 | 0.4769 |  0.4563 |  0.4380 |    0.4481 |
|     all-vanilla | 0.6381 |  0.4871 |       - |         - |
|-----------------+--------+---------+---------+-----------|

结论：noise的范围取0.7会有较好的效果。下面是dpr=0.7,noise=0.7时随着epoch变动而产生的精度变化：

|------------+--------+--------|
| eposide    |   BLEU | rougeL |
|------------+--------+--------|
| ep0        | 0.3820 | 0.4274 |
| ep1        | 0.4243 | 0.4366 |
| ep2        | 0.5131 | 0.4639 |
| ep3        | 0.4864 | 0.4528 |
| ep4        | 0.5064 | 0.4615 |
| finally    | 0.5064 | 0.4618 |
| valmin     | 0.3974 | 0.4222 |
| trainmodel | 0.4274 | 0.4389 |
|------------+--------+--------|

结论：至少要训练3个epoch。对于0.7这种高noise没有观察到过拟合问题

** Vary other training losses.

+ CE: vanilla crossEntropy
+ cosEm: consine embedding
+ mseEm: MSE embedding
+ SCE: tempratured CE
+ MSEh: MSE of hidden states

Hyperparamerters:
+ dropout=0.7
+ noise=0.7

  ckpt: finally

|-----------------+--------+---------+---------+-----------|
| loss type       |   BLEU | rouge-L | ep2BLEU | ep2rougeL |
|-----------------+--------+---------+---------+-----------|
| CE              | 0.1036 |  0.2582 |         |           |
| CE+cosEM        | 0.5064 |  0.4618 |  0.5131 |    0.4639 |
| CE+mseEM        | 0.3429 |  0.4002 |  0.3302 |    0.4094 |
| CE+cosem+negaem | 0.4626 |  0.4479 |  0.3234 |    0.3931 |
| All loss        | 0.2573 |  0.3523 |  0.2587 |    0.3538 |
|-----------------+--------+---------+---------+-----------|

** 验证quad近似的效果
*** vary epoch

|---------+--------+---------+---------|
| ep      |   BLEU | nist-mt | rouge-L |
|---------+--------+---------+---------|
| ep0     | 0.2127 |  2.6922 |  0.3563 |
| ep1     | 0.2185 |  2.8155 |  0.3766 |
| ep2     | 0.2489 |  3.2514 |  0.4014 |
| ep3     | 0.2796 |  3.9549 |  0.4126 |
| finally | 0.2663 |  3.3143 |  0.4116 |
| trainmo | 0.2775 |  3.7752 |  0.4091 |
|---------+--------+---------+---------|

** vary lamda

lamda是loss的权重，即 $L=(1-lamda)*otherloss+lamda*wordCosLoss*$

|-------+--------+--------|
| lamda |   bleu |   rouL |
|-------+--------+--------|
|  0.25 | 0.2489 | 0.3898 |
|   0.5 | 0.2568 | 0.3872 |
|   0.6 | 0.2667 | 0.4027 |
|  0.75 | 0.2907 | 0.4160 |
|   0.8 | 0.2756 | 0.4077 |
|  0.85 | 0.2922 | 0.4135 |
|   0.9 | 0.2543 | 0.3952 |
|-------+--------+--------|


* 总体结果表格
** WEB_NLG

|---------+--------+--------+---------+---------+--------+--------+--------+------|
| model   |   bleu | meteor |  chrf++ | nist_mt |   rou1 |   rou2 |   rouL | rouS |
|---------+--------+--------+---------+---------+--------+--------+--------+------|
| vanGPT2 | 0.5262 | 0.6558 | 68.9543 |  6.1507 | 0.7294 | 0.4979 | 0.6393 |      |
|---------+--------+--------+---------+---------+--------+--------+--------+------|
|         |        |        |         |         |        |        |        |      |

** E2E

|---------+--------+--------+---------+---------+--------+--------+--------+--------|
| model   |   bleu | meteor |  chrf++ | nist_mt |   rou1 |   rou2 |   rouL | rouSum |
|---------+--------+--------+---------+---------+--------+--------+--------+--------|
| vanGPT2 | 0.6381 | 0.6162 | 58.6091 |  5.9059 | 0.6791 | 0.4172 | 0.4871 | 0.4873 |
|---------+--------+--------+---------+---------+--------+--------+--------+--------|
| Our     | 0.5700 | 0.5489 | 52.2536 |  4.5914 | 0.6365 | 0.3709 | 0.4738 | 0.4737 |
|---------+--------+--------+---------+---------+--------+--------+--------+------|

** MultiWoz2.1 NLG

|---------+--------+--------+---------+---------+----------+--------+--------+--------|
| model   |   bleu | meteor |  chrf++ | nist_mt |     rou1 |   rou2 |   rouL | rouSum |
|---------+--------+--------+---------+---------+----------+--------+--------+--------|
| vanGPT2 | 0.3497 | 0.4887 | 43.3236 |  4.6580 | 0.0.5040 | 0.2730 | 0.4424 | 0.4424 |
| w. ER   | 0.0008 | 0.0425 |  3.9721 |  0.2327 |   0.0749 | 0.0029 | 0.0707 | 0.0708 |
|---------+--------+--------+---------+---------+----------+--------+--------+--------|
| onlyER  | 0.3309 | 0.5010 | 43.6481 |  4.6506 |   0.5126 | 0.2583 | 0.4520 | 0.4520 |
| Our-QC  | 0.2796 |        |         |  3.9549 |          |        | 0.4126 |        |
| SimLN   | 0.2568 | 0.4221 | 36.2575 |  3.5394 |   0.4433 | 0.1919 | 0.3872 | 0.3872 |
| QInter  | 0.2206 | 0.3874 | 32.5074 |  2.4881 |   0.4296 | 0.1811 | 0.3820 | 0.3820 |
| ConMat  | 0.2650 | 0.4329 | 36.3804 |  3.3641 |   0.4542 | 0.2068 | 0.4016 | 0.4015 |
| CM+SLN  | 0.2976 | 0.4585 | 39.4982 |  4.0478 |   0.4709 | 0.2264 | 0.4155 | 0.4154 |
|---------+--------+--------+---------+---------+----------+--------+--------+--------|
| T5van   |        |        |         |         |          |        |        |        |
|---------+--------+--------+---------+---------+----------+--------+--------+--------|

结论：
1. interKL loss不能加，加了效果反而不好
2. quad+cons 会带来较大的下降
3.

问题：
1. 能否测试一下使用正常的activation function，但是使用constant Matrix的推理速度和训练效果？

** Daily Dialog

|---------+--------+--------+---------+---------+--------+--------+--------+--------|
| model   |   bleu | meteor |  chrf++ | nist_mt |   rou1 |   rou2 |   rouL | rouSum |
|---------+--------+--------+---------+---------+--------+--------+--------+--------|
| vanGPT2 | 0.0155 | 0.1045 | 11.4873 |  0.5366 | 0.1227 | 0.0256 | 0.1136 | 0.1135 |
| w. ER   | 0.0078 | 0.0578 |  6.9919 |  0.1736 | 0.0692 | 0.0076 | 0.0628 | 0.0629 |
|---------+--------+--------+---------+---------+--------+--------+--------+--------|
| Our     | 0.0133 | 0.0916 | 10.5035 |  0.7336 | 0.1154 | 0.0152 | 0.1067 | 0.1063 |
|---------+--------+--------+---------+---------+--------+--------+--------+--------|

** Common Gen

|---------+--------+--------+---------+---------+--------+--------+--------+--------|
| model   |   bleu | meteor |  chrf++ | nist_mt |   rou1 |   rou2 |   rouL | rouSum |
|---------+--------+--------+---------+---------+--------+--------+--------+--------|
| vangpt  | 0.0707 | 0.2631 | 24.8923 |  1.7489 | 0.2395 | 0.0442 | 0.2036 | 0.2036 |
| Ours(7) |        |        |         |         |        |        |        |        |
|         |        |        |         |         |        |        |        |        |
|---------+--------+--------+---------+---------+--------+--------+--------+--------|
| vant5   | 0.1572 | 0.3313 | 29.6472 |  2.6676 | 0.3622 | 0.1094 | 0.3024 | 0.3028 |
|---------+--------+--------+---------+---------+--------+--------+--------+--------|
| vanbart | 0.1477 | 0.2938 | 27.6119 |  1.7859 | 0.3446 | 0.1027 | 0.2862 | 0.2863 |
|---------+--------+--------+---------+---------+--------+--------+--------+--------|



* 总体结果表格 new

** MultiWoz2.1 NLG

1. VanGPT: vanilla GPT-2
2. VanGPT+ER: vanilla GPT-2, ER generation
3. VanGPT+ER+cosLoss: vanilla GPT-2 with cosine loss and ER generation
4. VanGPT+ER+cosLoss+WeightedLoss: Vanilla GPT-2 with cosine loss and ER generation and the weighted loss function.
5. VanGPT+ER+cosLOss+CM+simLN: GPT2 with ER and cosine loss and constant matrix and simple layer normalization
6. VanGPT+ER+cosLoss+CM+simLN+QuadActivation:GPT2 with ER and cosine loss and constant matrix and simple layer normalization and Quad activation fuctions
7. VanGPT+ER+cosLoss+CM+simLN+QA+WL: above methods and the Weighted Loss

|-------------------------+--------+---------+---------+--------+--------+--------+--------+--------+---------+--------+---------|
| model                   |  Berts |   Barts | nist_mt |   rou1 |   rou2 |   rouL | rouSum | meteor |  chrf++ |   bleu |  BLEURT |
|-------------------------+--------+---------+---------+--------+--------+--------+--------+--------+---------+--------+---------|
| 1(vanG)                 | 0.9237 | -2.9020 |  4.7907 | 0.5040 | 0.2730 | 0.4424 | 0.4424 | 0.4900 | 43.2777 | 0.3603 | -0.2807 |
| 2(vanG+ER)              | 0.6860 | -5.0660 |  0.2325 | 0.0749 | 0.0029 | 0.0707 | 0.0708 | 0.0425 |  3.9721 | 0.0001 | -1.3474 |
|-------------------------+--------+---------+---------+--------+--------+--------+--------+--------+---------+--------+---------|
| MPCformer               |        |         |         |        |        |        |        |        |         |        |         |
| THE-X                   |        |         |         |        |        |        |        |        |         |        |         |
|-------------------------+--------+---------+---------+--------+--------+--------+--------+--------+---------+--------+---------|
| 3(vanG+ER+COS) [USE]    | 0.9094 | -2.8516 |  4.2540 | 0.4910 | 0.2403 | 0.4371 | 0.4376 | 0.4760 | 41.0684 | 0.3147 | -0.5577 |
| 6(VERCCS+QA)[USE]       | 0.8837 | -3.3335 |  3.6441 | 0.4084 | 0.1701 | 0.3545 | 0.3542 | 0.3855 | 32.8355 | 0.2355 |  3.3356 |
| 7(VERCWCS+QA)ours[USE]  | 0.9047 | -3.0474 |  4.1485 | 0.4748 | 0.2266 | 0.4162 | 0.4160 | 0.4570 | 39.0033 | 0.2998 | -0.5809 |
|-------------------------+--------+---------+---------+--------+--------+--------+--------+--------+---------+--------+---------|
| T5van                   |        |         |         |        |        |        |        |        |         |        |         |
|-------------------------+--------+---------+---------+--------+--------+--------+--------+--------+---------+--------+---------|


| 4(vG+ER+CS+WL)          | 0.9172 | -2.7433 |  4.8608 | 0.5216 | 0.2764 | 0.4645 | 0.4643 | 0.5139 | 44.7613 | 0.3639 | -0.4308 |
| 5(VGERddCSWLCMSLN) lmd.75 | 0.9037 | -3.0821 |  4.1129 | 0.4657 | 0.2201 | 0.4092 | 0.4091 | 0.4498 | 38.5829 | 0.2852 |         |

结论：
1. interKL loss不能加，加了效果反而不好
2. quad+cons 会带来较大的下降
3.

问题：
1. 能否测试一下使用正常的activation function，但是使用constant Matrix的推理速度和训练效果？


* appendix

** ideally e2e results, with dropout=0.4 and noise=0.7, finally

#+begin_src python
x={'bleu': {'bleu': 0.5001423160755862, 'precisions': [0.8378426592138907, 0.6267375054055724, 0.4315099312206724, 0.2761439003148657], 'brevity_penalty': 1.0, 'length_ratio': 1.1628405476420964, 'translation_length': 16817, 'reference_length': 14462}, 'meteor': {'meteor': 0.5491321033526948}, 'chrf': {'score': 52.58029557222124, 'char_order': 6, 'word_order': 0, 'beta': 2}, 'ter': {'res': 'empty, with error.'}, 'nist_mt': {'nist_mt': 5.391523328228687}, 'rouge': {'rouge1': 0.6216837886970417, 'rouge2': 0.3479140847535767, 'rougeL': 0.45397995032307703, 'rougeLsum': 0.4541240112465835}}
#+end_src

** ideally e2e results, with dropout=0.4 and noise=0.7, epoch num=2
#+begin_src python
  {'bleu': {'bleu': 0.41036574078963006, 'precisions': [0.7738218539616779, 0.5477797273586179, 0.3425429940795038, 0.19530904263165658], 'brevity_penalty': 1.0, 'length_ratio': 1.0681786751486655, 'translation_length': 15448, 'reference_length': 14462}, 'meteor': {'meteor': 0.4835677578823461}, 'chrf': {'score': 49.54960270866937, 'char_order': 6, 'word_order': 0, 'beta': 2}, 'ter': {'res': 'empty, with error.'}, 'nist_mt': {'nist_mt': 4.269701183206952}, 'rouge': {'rouge1': 0.5681178949386517, 'rouge2': 0.3010242308061636, 'rougeL': 0.42038681566486646, 'rougeLsum': 0.4208281098387816}}
  
#+end_src

** ideally e2e results, with dropout=0.7 and noise=0.7, finally

#+begin_src python
  {'bleu': {'bleu': 0.506388367450128, 'precisions': [0.855787476280835, 0.6310349709755062, 0.4342027267338471, 0.28042903777397793], 'brevity_penalty': 1.0, 'length_ratio': 1.020329138431752, 'translation_length': 14756, 'reference_length': 14462}, 'meteor': {'meteor': 0.5284362390061123}, 'chrf': {'score': 51.10534178683519, 'char_order': 6, 'word_order': 0, 'beta': 2}, 'ter': {'res': 'empty, with error.'}, 'nist_mt': {'nist_mt': 4.36497366046108}, 'rouge': {'rouge1': 0.6195600596733838, 'rouge2': 0.3483983400112336, 'rougeL': 0.4618130443227314, 'rougeLsum': 0.4620051694249079}}
#+end_src

** ideally e2e results, with dropout=0.7 and noise=0.7, train_epoch=2

#+begin_src python
{'bleu': {'bleu': 0.5130692346678581, 'precisions': [0.8583251873574455, 0.6392796466190962, 0.4433084842030529, 0.2848755109624675], 'brevity_penalty': 1.0, 'length_ratio': 1.0610565620246162, 'translation_length': 15345, 'reference_length': 14462}, 'meteor': {'meteor': 0.5369476938218365}, 'chrf': {'score': 51.94087977811228, 'char_order': 6, 'word_order': 0, 'beta': 2}, 'ter': {'res': 'empty, with error.'}, 'nist_mt': {'nist_mt': 4.715776570192349}, 'rouge': {'rouge1': 0.6230304335515794, 'rouge2': 0.3506989030961768, 'rougeL': 0.4637012349452837, 'rougeLsum': 0.4638031470932258}}  
#+end_src

** e2d dr0.7 noise0.7 trainep2 add filter

#+begin_src 
 {'bleu': {'bleu': 0.570037123026437, 'precisions': [0.8940864960282436, 0.6817504787573587, 0.49944316578810605, 0.34683386556585405], 'brevity_penalty': 1.0, 'length_ratio': 1.0184621767390403, 'translation_length': 14729, 'reference_length': 14462}, 'meteor': {'meteor': 0.5488816053515401}, 'chrf': {'score': 52.25359798020093, 'char_order': 6, 'word_order': 0, 'beta': 2}, 'ter': {'res': 'empty, with error.'}, 'nist_mt': {'nist_mt': 4.5913598139149725}, 'rouge': {'rouge1': 0.6365187866109462, 'rouge2': 0.37091224228433994, 'rougeL': 0.4738310154658901, 'rougeLsum': 0.4736721774658565}}
#+end_src

** ER dailydialog res

#+begin_src python
{'bleu': {'bleu': 0.0486916436271834, 'precisions': [0.23657965484346544, 0.06763523520699626, 0.04506955495097788, 0.03594789669561295], 'brevity_penalty': 0.6823821321376728, 'length_ratio': 0.7235023764333108, 'translation_length': 46124, 'reference_length': 63751}, 'meteor': {'meteor': 0.13693178391745225}, 'chrf': {'score': 15.463535023859858, 'char_order': 6, 'word_order': 0, 'beta': 2}, 'ter': {'res': 'empty, with error.'}, 'nist_mt': {'nist_mt': 1.1967955462313036}, 'rouge': {'rouge1': 0.1593568502920759, 'rouge2': 0.05939760931492541, 'rougeL': 0.1480622690035906, 'rougeLsum': 0.14781708252035197}}  
#+end_src













* New appdendix

** multiwoz nlg

*** t5 original

{'bleu': {'bleu': 0.3188210300794158, 'precisions': [0.5232476451275287, 0.36435085638021797, 0.272415964128315, 0.19894279300078016], 'brevity_penalty': 1.0, 'length_ratio': 1.253689224409095, 'translation_length': 69749, 'reference_length': 55635}, 'meteor': {'meteor': 0.5225416548176837}, 'chrf': {'score': 45.02296440375493, 'char_order': 6, 'word_order': 0, 'beta': 2}, 'ter': {'res': 'empty, with error.'}, 'nist_mt': {'nist_mt': 4.243508240083943}, 'rouge': {'rouge1': 0.4846778941780512, 'rouge2': 0.25982614415796634, 'rougeL': 0.4215551156167472, 'rougeLsum': 0.4218329549818247}, 'bleurt': -0.6187268173822758, 'bert_score': tensor(0.9140), 'bart_score': -2.8915836795353127}
